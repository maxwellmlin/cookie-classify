{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a modified version of `tracker_analysis.ipynb` meant to analyze OneTrust compliance.\n",
    "\n",
    "Specifically, crawl websites that use the OneTrust CMP. By modifiying the `OptanonConsent` cookie, reject only the tracking cookies while accepting all other cookies. A website is *not* compliant if tracking cookies are still detected. Otherwise the website is compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import csv\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from crawler import CMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Analysis and Creating Blocklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"analysis\"):\n",
    "    os.mkdir(\"analysis\")\n",
    "\n",
    "def get_tracking_domains(list_path: str = \"inputs/blocklists/\") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get tracking domains from blocklists.\n",
    "\n",
    "    Args:\n",
    "        list_path: Path to blocklists. Defaults to \"inputs/blocklists/\".\n",
    "\n",
    "    Returns:\n",
    "        A set of tracking domains.\n",
    "    \"\"\"\n",
    "    lists = []\n",
    "    for item in os.listdir(list_path):\n",
    "        path = os.path.join(list_path, item)\n",
    "        lists.append(path)\n",
    "\n",
    "    tracking_sites = set()\n",
    "    for list_path in lists:\n",
    "        with open(list_path) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                tracking_sites.add(line.rstrip())\n",
    "\n",
    "    # print(\"Tracking sites aggregated from 4 blocklists.\")\n",
    "    return tracking_sites\n",
    "\n",
    "# Create set of tracking domains from aggregation of 4 blocklists\n",
    "trackings_domains = get_tracking_domains()\n",
    "print(trackings_domains)\n",
    "\n",
    "def get_directories(root: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of directories in a given root directory.\n",
    "\n",
    "    Args:\n",
    "        root: Path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of directories.\n",
    "    \"\"\"\n",
    "    dirs = []\n",
    "    for item in os.listdir(root):\n",
    "        path = os.path.join(root, item)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(path)\n",
    "\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def detect_tracking(blocklist, cookie_list) -> list[dict[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Check if any URLs from a list appear in a blocklist of known tracking cookies.\n",
    "\n",
    "    Args:\n",
    "        blocklist: Set of blocked domains.\n",
    "        cookie_list: List of cookies, where each cookie is a dict of 3 key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        A filtered list of detected tracking cookies.\n",
    "    \"\"\"\n",
    "\n",
    "    detected_trackers = []\n",
    "    for cookie in cookie_list:\n",
    "        cookie_domain = cookie[\"Cookie Domain\"]\n",
    "        if utils.get_domain(cookie_domain) in blocklist:\n",
    "            detected_trackers.append(cookie)\n",
    "\n",
    "    return detected_trackers\n",
    "\n",
    "\n",
    "def get_cookies_from_har(file: str) -> list[dict[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Returns a list of cookies from response entries in an HAR file.\n",
    "    [HAR Specification](http://www.softwareishard.com/blog/har-12-spec/).\n",
    "\n",
    "    Args:\n",
    "        file: Path to the HAR file.\n",
    "    Returns:\n",
    "        A list of dictionaries representing all cookies in HTTP responses in that HAR file with domains, where each dictionary holds 3 key-value pairs (Cookie Name, Cookie Value, Cookie Domain).\n",
    "    \"\"\"\n",
    "\n",
    "    cookies = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        \n",
    "        response = entry[\"response\"] # extract response dictionary\n",
    "\n",
    "        if response.get(\"cookies\"): # response contains cookies\n",
    "            for cookie in response[\"cookies\"]:\n",
    "                # print(cookie)\n",
    "                if cookie.get(\"domain\"): # if cookie has domain\n",
    "                    cookies.append({\"Cookie Name\": cookie[\"name\"], \"Cookie Value\": cookie[\"value\"], \"Cookie Domain\": cookie[\"domain\"]})\n",
    "\n",
    "    return cookies\n",
    "\n",
    "def check_requests(detected_list_from_responses: list[dict[str, str, str]], file: str) -> list[dict[str, str, str]]:\n",
    "    \n",
    "    detected_list_from_requests = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        \n",
    "        request = entry[\"request\"] # extract request dictionary\n",
    "\n",
    "        for cookie in request.get(\"cookies\"):\n",
    "            values_of_cookie_names = [d[\"Cookie Name\"] for d in detected_list_from_responses]\n",
    "            if cookie.get(\"name\") in values_of_cookie_names: # if cookie name is in list of detected cookies from responses\n",
    "                detected_list_from_requests.append({\"Cookie Name\": cookie[\"name\"], \"Cookie Value\": cookie[\"value\"]})\n",
    "\n",
    "    return detected_list_from_requests\n",
    "\n",
    "def analyze_har(har_path: str) -> list[dict[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Return a list of tracking cookies detected in the requests of a specified HAR file.\n",
    "\n",
    "    Args:\n",
    "        har_path: Path to the HAR file.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries representing detected tracking cookies from requests, where each dictionary holds 3 key-value pairs (Cookie Name, Cookie Value, Cookie Domain).\n",
    "    \"\"\"\n",
    "    cookies = get_cookies_from_har(har_path)\n",
    "    filtered_list = detect_tracking(trackings_domains, cookies)\n",
    "    return filtered_list\n",
    "\n",
    "# print(get_cookies_from_har(\"crawls/depth0/bmj.com/0/normal.json\"))\n",
    "# print(analyze_har(\"crawls/depth0/bmj.com/0/normal.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes and Generate CSV Files\n",
    "Note: Running this cell block will append lines to existing CSV files. Delete existing CSV files or comment out lines before each new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CRAWL_NAME = \"aug30-onetrust\"\n",
    "\n",
    "domain_paths = get_directories(f\"crawls/{CRAWL_NAME}\") \n",
    "successful_domains = []\n",
    "with open(f\"crawls/{CRAWL_NAME}/results.json\") as log_file:\n",
    "    results = json.load(log_file)\n",
    "    for path in results:\n",
    "        if results[path][\"interact_success\"] and results[path][\"interact_type\"] == \"OneTrust\":\n",
    "            site = os.path.basename(os.path.normpath(path))\n",
    "            successful_domains.append(site)\n",
    "\n",
    "# for counting number of inner pages per domain\n",
    "domains_paths_normal = {}\n",
    "domains_paths_reject = {}\n",
    "\n",
    "incomplete_runs = 0\n",
    "total_inner_pages = 0\n",
    "\n",
    "detected_trackers_from_responses_normal = []\n",
    "detected_trackers_from_requests_normal = [] # will be used to create DataFrame\n",
    "\n",
    "detected_trackers_from_responses_reject = []\n",
    "detected_trackers_from_requests_reject = [] # will be used to create Dataframe\n",
    "\n",
    "for site in domain_paths:\n",
    "    # Skip if site is not in success.txt\n",
    "    # FIXME: success.txt currently not formatted properly; uncommenting this causes no rows to be written to CSV\n",
    "    # if not any(site in line for line in success_lines):\n",
    "    #     continue\n",
    "\n",
    "    inner_site_paths = get_directories(site)\n",
    "    total_inner_pages += len(inner_site_paths)\n",
    "\n",
    "    for inner_site_path in inner_site_paths:\n",
    "        normal_har_path = f\"{inner_site_path}/no_interaction.json\"\n",
    "        reject_har_path = f\"{inner_site_path}/reject_only_tracking.json\"\n",
    "\n",
    "        if not os.path.isfile(normal_har_path) or not os.path.isfile(reject_har_path):\n",
    "            # Requires both normal and intercept HAR files to exist\n",
    "            incomplete_runs += 1\n",
    "            continue\n",
    "            \n",
    "        domain = site.split(\"/\")[2]\n",
    "\n",
    "        # Append inner site path to the dictionary for normal crawls\n",
    "        if domain in domains_paths_normal:\n",
    "            domains_paths_normal[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_normal[domain] = [inner_site_path]\n",
    "\n",
    "        # Append inner site path to the dictionary for after_reject crawls\n",
    "        if domain in domains_paths_reject:\n",
    "            domains_paths_reject[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_reject[domain] = [inner_site_path]\n",
    "\n",
    "        detected_list_from_requests_normal = analyze_har(normal_har_path)\n",
    "\n",
    "        # saving trackers from responses for easy parsing into dataframe if needed\n",
    "        for detected_tracker in detected_list_from_requests_normal:\n",
    "            detected_trackers_from_responses_normal.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "                \"Cookie Domain\": detected_tracker[\"Cookie Domain\"]\n",
    "            })\n",
    "\n",
    "        trackers_requests_normal = check_requests(detected_trackers_from_responses_normal, normal_har_path)\n",
    "        \n",
    "        for detected_tracker in trackers_requests_normal:\n",
    "            detected_trackers_from_requests_normal.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "                # TODO: maybe we can use reqeust.url to get the domain?\n",
    "            })\n",
    "\n",
    "        # # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        # normal_file = \"analysis/depth1_normal.csv\"\n",
    "        # normal_file_exists = os.path.isfile(normal_file)\n",
    "\n",
    "        # if normal_file_exists:\n",
    "        #     with open(normal_file, mode=\"a\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_normal)])\n",
    "        #         file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        # else:\n",
    "        #     with open(normal_file, mode=\"w\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_normal)])\n",
    "        #         file.flush()\n",
    "\n",
    "\n",
    "        # Repeat for files generated after run with rejecting cookies\n",
    "        detected_list_from_requests_reject = analyze_har(reject_har_path)\n",
    "\n",
    "        # saving trackers from responses for easy parsing into dataframe if needed\n",
    "        for detected_tracker in detected_list_from_requests_reject:\n",
    "            detected_trackers_from_responses_reject.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "                \"Cookie Domain\": detected_tracker[\"Cookie Domain\"]\n",
    "            })\n",
    "\n",
    "        trackers_requests_reject = check_requests(detected_trackers_from_responses_reject, reject_har_path)\n",
    "        \n",
    "        for detected_tracker in trackers_requests_reject:\n",
    "            detected_trackers_from_requests_reject.append({\n",
    "                \"Domain\": domain,\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Cookie Name\": detected_tracker[\"Cookie Name\"],\n",
    "                \"Cookie Value\": detected_tracker[\"Cookie Value\"],\n",
    "            })\n",
    "\n",
    "        # # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        # reject_file = \"analysis/depth1_after_reject.csv\"\n",
    "        # reject_file_exists = os.path.isfile(reject_file)\n",
    "\n",
    "        # if reject_file_exists:\n",
    "        #     with open(reject_file, mode=\"a\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_reject)])\n",
    "        #         file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        # else:\n",
    "        #     with open(reject_file, mode=\"w\", newline=\"\") as file:\n",
    "        #         writer = csv.writer(file)\n",
    "        #         writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "        #         writer.writerow([inner_site_path, len(trackers_requests_reject)])\n",
    "        #         file.flush()\n",
    "\n",
    "\n",
    "# Create DataFrames for detected trackers in normal and after_reject crawls\n",
    "# Each tracker is in a row, with its domain and inner site path\n",
    "df_normal = pd.DataFrame(detected_trackers_from_requests_normal)\n",
    "df_after_reject = pd.DataFrame(detected_trackers_from_requests_reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.info()\n",
    "# df_after_reject.info()\n",
    "# df_normal.head(15)\n",
    "# df_after_reject.head(15)\n",
    "# df_normal.to_csv(\"analysis/depth1_normal_1.csv\")\n",
    "# df_after_reject.to_csv(\"analysis/depth1_after_reject_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Trackers that Remain After Rejecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in df_normal (if every value in a row is the same, it is considered a duplicate)\n",
    "df_normal_unique = df_normal.drop_duplicates()\n",
    "\n",
    "# Perform an inner merge (cookies in df_after_reject are kept if they are in df_normal_unique)\n",
    "merged_df = df_after_reject.merge(df_normal_unique, on=[\"Domain\", \"Inner Site Path\", \"Cookie Name\", \"Cookie Value\"], how=\"inner\")\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_normal\n",
    "df_normal_domains = df_normal.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_normal.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_normal dictionary\n",
    "    Num_Trackers_Per_Domain_No_Interaction=('Cookie Name', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_normal_domains[\"Average Trackers Per Page\"] = df_normal_domains[\"Num_Trackers_Per_Domain_No_Interaction\"] / df_normal_domains[\"Num_Inner_Pages\"]\n",
    "\n",
    "\n",
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_reject\n",
    "df_after_reject_domains = df_after_reject.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_reject.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_reject dictionary\n",
    "    Num_Trackers_Per_Domain_Disable_Only_Tracking=('Cookie Name', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_after_reject_domains[\"Average Trackers Per Page\"] = df_after_reject_domains[\"Num_Trackers_Per_Domain_Disable_Only_Tracking\"] / df_after_reject_domains[\"Num_Inner_Pages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dfs and compare different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_domains = pd.DataFrame(successful_domains, columns =['Domain'])\n",
    "\n",
    "merged_df = pd.merge(df_normal_domains[[\"Domain\", \"Num_Trackers_Per_Domain_No_Interaction\"]], df_after_reject_domains[[\"Domain\", \"Num_Trackers_Per_Domain_Disable_Only_Tracking\"]], on='Domain', how='outer')\n",
    "merged_df = pd.merge(merged_df, successful_domains, on='Domain', how='outer')\n",
    "merged_df.fillna(0, inplace=True)\n",
    "merged_df.to_csv(f\"analysis/{CRAWL_NAME}-merged.csv\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for (_, row) in merged_df.iterrows():\n",
    "    if row.Num_Trackers_Per_Domain_Disable_Only_Tracking > 0:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for (_, row) in merged_df.iterrows():\n",
    "    if row.Num_Trackers_Per_Domain_No_Interaction > 0:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.info()\n",
    "# df_normal_domains.info()\n",
    "df_normal_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_after_reject_domains.info()\n",
    "df_after_reject_domains\n",
    "# df_after_reject.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Trackers Kept After Rejecting, Grouped by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"Domain\" and count the number of trackers for each domain\n",
    "domain_counts = merged_df.groupby(\"Domain\").size().reset_index(name=\"Tracker Count\")\n",
    "\n",
    "# Sort the DataFrame by descending \"Tracker Count\"\n",
    "domain_counts_sorted = domain_counts.sort_values(by=\"Tracker Count\", ascending=False)\n",
    "# domain_counts\n",
    "domain_counts_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookie-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
