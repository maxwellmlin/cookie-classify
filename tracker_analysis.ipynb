{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import csv\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files for analysis\n",
    "Note: Running this cell block will append lines to existing CSV files. Delete existing CSV files before each new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394 entries, 0 to 393\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Domain           394 non-null    object\n",
      " 1   Inner Site Path  394 non-null    object\n",
      " 2   Tracker          394 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 373 entries, 0 to 372\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Domain           373 non-null    object\n",
      " 1   Inner Site Path  373 non-null    object\n",
      " 2   Tracker          373 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 8.9+ KB\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"analysis\"):\n",
    "    os.mkdir(\"analysis\")\n",
    "\n",
    "\n",
    "def detect_tracking(blocklist, url_list):\n",
    "    \"\"\"\n",
    "    Check if any URLs from a list appear in a blocklist of known tracking cookies.\n",
    "\n",
    "    Args:\n",
    "        blocklist: Set of blocked domains.\n",
    "        url_list: List of URLs.\n",
    "\n",
    "    Returns:\n",
    "        A list of detected trackers.\n",
    "    \"\"\"\n",
    "\n",
    "    detected_trackers = []\n",
    "    for url in url_list:\n",
    "        if utils.get_full_domain(url) in blocklist: # FIXME: check if this is the correct way to get domain\n",
    "            detected_trackers.append(url)\n",
    "\n",
    "    return detected_trackers\n",
    "\n",
    "\n",
    "def get_urls_from_har(file: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of cookies from an HAR file.\n",
    "    [HAR Specification](http://www.softwareishard.com/blog/har-12-spec/).\n",
    "\n",
    "    Args:\n",
    "        file: Path to the HAR file.\n",
    "    Returns:\n",
    "        A list of cookies.\n",
    "    \"\"\"\n",
    "\n",
    "    all_urls = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        request = entry[\"request\"] # extract request dictionary\n",
    "\n",
    "        if (url := request.get(\"url\")) and request.get(\"cookies\"): # valid URL exists and request contains cookies\n",
    "            all_urls.append(url)\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "\n",
    "def get_tracking_sites(list_path: str = \"inputs/blocklists/\") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get tracking sites from blocklists.\n",
    "\n",
    "    Args:\n",
    "        list_path: Path to blocklists. Defaults to \"inputs/blocklists/\".\n",
    "\n",
    "    Returns:\n",
    "        A set of tracking sites.\n",
    "    \"\"\"\n",
    "    lists = []\n",
    "    for item in os.listdir(list_path):\n",
    "        path = os.path.join(list_path, item)\n",
    "        lists.append(path)\n",
    "\n",
    "    tracking_sites = set()\n",
    "    for list_path in lists:\n",
    "        with open(list_path) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                tracking_sites.add(line.rstrip())\n",
    "\n",
    "    # print(\"Tracking sites aggregated from 4 blocklists.\")\n",
    "    return tracking_sites\n",
    "\n",
    "\n",
    "def get_directories(root: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of directories in a given root directory.\n",
    "\n",
    "    Args:\n",
    "        root: Path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of directories.\n",
    "    \"\"\"\n",
    "    dirs = []\n",
    "    for item in os.listdir(root):\n",
    "        path = os.path.join(root, item)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(path)\n",
    "\n",
    "    return dirs\n",
    "\n",
    "\n",
    "# Create set of tracking sites from aggregation of 4 blocklists\n",
    "trackings_sites = get_tracking_sites()\n",
    "# print(trackings_sites)\n",
    "\n",
    "\n",
    "def analyze_har(har_path: str):\n",
    "    \"\"\"\n",
    "    Return a list of tracking cookies detected in the specified HAR file.\n",
    "\n",
    "    Args:\n",
    "        har_path: Path to the HAR file.\n",
    "\n",
    "    Returns:\n",
    "        A list of detected tracking cookies.\n",
    "    \"\"\"\n",
    "    urls = get_urls_from_har(har_path) # get list of URLs\n",
    "    detected_list = detect_tracking(trackings_sites, urls)\n",
    "    return detected_list\n",
    "\n",
    "\n",
    "success_file_path = \"inputs/sites/success.txt\"\n",
    "with open(success_file_path, \"r\") as success_file:\n",
    "    success_lines = success_file.readlines()\n",
    "\n",
    "# TODO: change filepath\n",
    "# domain_paths = get_directories(\"crawls/depth1_noquery\") \n",
    "domain_paths = get_directories(\"crawls/depth0\") \n",
    "\n",
    "# Initialize dictionaries to store inner site paths for normal and after_reject crawls\n",
    "domains_paths_normal = {}\n",
    "domains_paths_reject = {}\n",
    "\n",
    "incomplete_runs = 0\n",
    "total_inner_pages = 0\n",
    "\n",
    "detected_trackers_normal = []\n",
    "detected_trackers_after_reject = []\n",
    "\n",
    "for site in domain_paths:\n",
    "    # Skip if site is not in success.txt\n",
    "    # FIXME: success.txt currently not formatted properly; uncommenting this causes no rows to be written to CSV\n",
    "    # if not any(site in line for line in success_lines):\n",
    "    #     continue\n",
    "\n",
    "    inner_site_paths = get_directories(site)\n",
    "    total_inner_pages += len(inner_site_paths)\n",
    "\n",
    "    for inner_site_path in inner_site_paths:\n",
    "        normal_har_path = f\"{inner_site_path}/normal.json\"\n",
    "        reject_har_path = f\"{inner_site_path}/after_reject.json\"\n",
    "\n",
    "        if not os.path.isfile(normal_har_path) or not os.path.isfile(reject_har_path):\n",
    "            # Requires both normal and intercept HAR files to exist\n",
    "            incomplete_runs += 1\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        domain = site.split(\"/\")[2]\n",
    "        \n",
    "        # Append inner site path to the dictionary for normal crawls\n",
    "        if domain in domains_paths_normal:\n",
    "            domains_paths_normal[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_normal[domain] = [inner_site_path]\n",
    "\n",
    "        # Append inner site path to the dictionary for after_reject crawls\n",
    "        if domain in domains_paths_reject:\n",
    "            domains_paths_reject[domain].append(inner_site_path)\n",
    "        else:\n",
    "            domains_paths_reject[domain] = [inner_site_path]\n",
    "\n",
    "        detected_list_normal = analyze_har(normal_har_path)\n",
    "\n",
    "\n",
    "        for tracker_url in detected_list_normal:\n",
    "            detected_trackers_normal.append({\n",
    "                \"Domain\": site.split(\"/\")[2],\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Tracker\": tracker_url\n",
    "            })\n",
    "\n",
    "        # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        # TODO: change name\n",
    "        # normal_file = \"analysis/depth1_noquery_normal.csv\"\n",
    "        normal_file = \"analysis/depth0_normal.csv\"\n",
    "        normal_file_exists = os.path.isfile(normal_file)\n",
    "\n",
    "        if normal_file_exists:\n",
    "            with open(normal_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([inner_site_path, len(detected_list_normal)])\n",
    "                file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        else:\n",
    "            with open(normal_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "                writer.writerow([inner_site_path, len(detected_list_normal)])\n",
    "                file.flush()\n",
    "\n",
    "\n",
    "        # Repeat for files generated after run with intercept.\n",
    "        detected_list_reject = analyze_har(reject_har_path)\n",
    "\n",
    "        for tracker_url in detected_list_reject:\n",
    "            detected_trackers_after_reject.append({\n",
    "                \"Domain\": site.split(\"/\")[2],\n",
    "                \"Inner Site Path\": inner_site_path,\n",
    "                \"Tracker\": tracker_url\n",
    "            })\n",
    "\n",
    "        # TODO: change name\n",
    "        # reject_file = \"analysis/depth1_noquery_after_reject.csv\"\n",
    "        reject_file = \"analysis/depth0_after_reject.csv\"\n",
    "        reject_file_exists = os.path.isfile(reject_file)\n",
    "\n",
    "        if reject_file_exists:\n",
    "            with open(reject_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([inner_site_path, len(detected_list_reject)])\n",
    "                file.flush()\n",
    "        else:\n",
    "            with open(reject_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "                writer.writerow([inner_site_path, len(detected_list_reject)])\n",
    "                file.flush()\n",
    "\n",
    "# Create DataFrames for detected trackers in normal and after_reject crawls\n",
    "# Each tracker is in a row with its domain and inner site path\n",
    "df_normal = pd.DataFrame(detected_trackers_normal)\n",
    "df_after_reject = pd.DataFrame(detected_trackers_after_reject)\n",
    "\n",
    "df_normal.info()\n",
    "df_after_reject.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal.tail(15)\n",
    "# df_after_reject.tail(15)\n",
    "\n",
    "# Check if the entire DataFrame contains null values\n",
    "if df_normal.isnull().any().any():\n",
    "    print(\"df_normal contains null values.\")\n",
    "\n",
    "if df_after_reject.isnull().any().any():\n",
    "    print(\"df_after_reject contains null values.\")\n",
    "\n",
    "if df_normal['Tracker'].isnull().any():\n",
    "    print(\"Tracker col contains null values.\")\n",
    "\n",
    "\n",
    "# df_normal_stackoverflow = df_normal.loc[df_normal['Domain'] == 'stackoverflow.com']\n",
    "# print(df_normal_stackoverflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_normal\n",
    "df_normal_domains = df_normal.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_normal.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_normal dictionary\n",
    "    Num_Trackers_Per_Domain=('Tracker', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_normal_domains[\"Average Trackers Per Page\"] = df_normal_domains[\"Num_Trackers_Per_Domain\"] / df_normal_domains[\"Num_Inner_Pages\"]\n",
    "\n",
    "\n",
    "# Group by domain and set Num_Inner_Pages using the dictionary domains_paths_reject\n",
    "df_after_reject_domains = df_after_reject.groupby('Domain', as_index=False).agg(\n",
    "    Num_Inner_Pages=('Domain', lambda x: len(domains_paths_reject.get(x.iloc[0], []))), # Use the length of inner site paths in the domains_paths_reject dictionary\n",
    "    Num_Trackers_Per_Domain=('Tracker', 'count')  # Count the number of trackers for each domain\n",
    ")\n",
    "\n",
    "df_after_reject_domains[\"Average Trackers Per Page\"] = df_after_reject_domains[\"Num_Trackers_Per_Domain\"] / df_after_reject_domains[\"Num_Inner_Pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Num_Inner_Pages</th>\n",
       "      <th>Num_Trackers_Per_Domain</th>\n",
       "      <th>Average Trackers Per Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aap.org</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acer.com</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>active.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adtelligent.com</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adweek.com</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>wufoo.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>xoom.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>yieldmo.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>zebra.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>zoom.us</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Domain  Num_Inner_Pages  Num_Trackers_Per_Domain  \\\n",
       "0           aap.org                1                        1   \n",
       "1          acer.com                1                        8   \n",
       "2        active.com                1                        2   \n",
       "3   adtelligent.com                1                        6   \n",
       "4        adweek.com                1                        4   \n",
       "..              ...              ...                      ...   \n",
       "77        wufoo.com                1                        2   \n",
       "78         xoom.com                1                        2   \n",
       "79      yieldmo.com                1                        1   \n",
       "80        zebra.com                1                        2   \n",
       "81          zoom.us                1                        3   \n",
       "\n",
       "    Average Trackers Per Page  \n",
       "0                         1.0  \n",
       "1                         8.0  \n",
       "2                         2.0  \n",
       "3                         6.0  \n",
       "4                         4.0  \n",
       "..                        ...  \n",
       "77                        2.0  \n",
       "78                        2.0  \n",
       "79                        1.0  \n",
       "80                        2.0  \n",
       "81                        3.0  \n",
       "\n",
       "[82 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_normal.info()\n",
    "# df_normal_domains.info()\n",
    "df_normal_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Num_Inner_Pages</th>\n",
       "      <th>Num_Trackers_Per_Domain</th>\n",
       "      <th>Average Trackers Per Page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aap.org</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acer.com</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>active.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adtelligent.com</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adweek.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>wish.com</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>wufoo.com</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>xoom.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>yieldmo.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>zoom.us</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Domain  Num_Inner_Pages  Num_Trackers_Per_Domain  \\\n",
       "0           aap.org                1                        1   \n",
       "1          acer.com                1                        8   \n",
       "2        active.com                1                        1   \n",
       "3   adtelligent.com                1                        3   \n",
       "4        adweek.com                1                        1   \n",
       "..              ...              ...                      ...   \n",
       "85         wish.com                1                        6   \n",
       "86        wufoo.com                1                        3   \n",
       "87         xoom.com                1                        2   \n",
       "88      yieldmo.com                1                        1   \n",
       "89          zoom.us                1                        5   \n",
       "\n",
       "    Average Trackers Per Page  \n",
       "0                         1.0  \n",
       "1                         8.0  \n",
       "2                         1.0  \n",
       "3                         3.0  \n",
       "4                         1.0  \n",
       "..                        ...  \n",
       "85                        6.0  \n",
       "86                        3.0  \n",
       "87                        2.0  \n",
       "88                        1.0  \n",
       "89                        5.0  \n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# df_after_reject_domains.info()\n",
    "df_after_reject_domains\n",
    "# df_after_reject.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Tracking Cookies Across All Inner Pages (Regardless of Domain)\n",
    "Run this cell to check that number of complete+incomplete pages equals total inner pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inner pages: 252\n",
      "Incomplete inner pages: 2\n",
      "Complete inner pages: 250\n",
      "Inner pages that removed all trackers after rejection: 8\n",
      "Inner pages with increased trackers after rejection: 33\n",
      "Inner pages that never contained trackers: 152\n",
      "Inner pages that sent cookies to 3rd party trackers after rejection: 90\n"
     ]
    }
   ],
   "source": [
    "def compare_trackers(reject_filepath, normal_filepath):\n",
    "    no_trackers_after_reject = []  # List of inner site paths with trackers in normal crawl, but no trackers after rejection\n",
    "    increased_trackers = []  # List of inner site paths with more trackers after rejection than in normal crawl\n",
    "    never_trackers = []  # List of inner site paths with no trackers in either normal or rejection crawl\n",
    "    violating_sites = []  # List of inner site paths with trackers after we click the reject button\n",
    "\n",
    "    with open(reject_filepath, 'r') as reject_file, open(normal_filepath, 'r') as normal_file:\n",
    "        read_reject = csv.reader(reject_file)\n",
    "        read_normal = csv.reader(normal_file)\n",
    "\n",
    "        # Skip header\n",
    "        next(read_reject)\n",
    "        next(read_normal)\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        # Since both csvs are sorted by inner site path, we can just iterate through both at the same time\n",
    "        for normal, after_reject in zip(read_normal, read_reject):\n",
    "            inner_site_path, num_trackers_normal = normal\n",
    "            _, num_trackers_reject = after_reject\n",
    "\n",
    "            if inner_site_path != _:\n",
    "                raise RuntimeError(\"Inner site paths do not match\")\n",
    "\n",
    "            num_trackers_normal = int(num_trackers_normal)\n",
    "            num_trackers_reject = int(num_trackers_reject)\n",
    "\n",
    "            if num_trackers_normal > 0 and num_trackers_reject == 0:  # if there are trackers in normal crawl, but not after reject\n",
    "                no_trackers_after_reject.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal < num_trackers_reject:  # if there are more trackers after reject than in normal crawl\n",
    "                increased_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal == 0 and num_trackers_reject == 0:  # if there are no trackers in either normal or reject\n",
    "                never_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_reject != 0:  # if there are trackers in reject\n",
    "                violating_sites.append(inner_site_path)\n",
    "\n",
    "            length += 1\n",
    "\n",
    "    # from previous cell\n",
    "    print(\"Total inner pages:\", total_inner_pages)\n",
    "    print(\"Incomplete inner pages:\", incomplete_runs)\n",
    "    \n",
    "    print(\"Complete inner pages:\", length)\n",
    "    print(\"Inner pages that removed all trackers after rejection:\", len(no_trackers_after_reject))\n",
    "    print(\"Inner pages with increased trackers after rejection:\", len(increased_trackers))\n",
    "    print(\"Inner pages that never contained trackers:\", len(never_trackers))\n",
    "    print(\"Inner pages that sent cookies to 3rd party trackers after rejection:\", len(violating_sites))\n",
    "\n",
    "\n",
    "def get_length_detected_list(csv_reader, inner_site_path):\n",
    "    for row in csv_reader:\n",
    "        current_inner_site_path, length_detected_list = row\n",
    "        if current_inner_site_path == inner_site_path:\n",
    "            return length_detected_list\n",
    "\n",
    "    return '0'  # If inner_site_path not found, return '0'\n",
    "\n",
    "\n",
    "compare_trackers('analysis/depth0_after_reject.csv', 'analysis/depth0_normal.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookie-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
