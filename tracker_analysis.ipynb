{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import utils\n",
    "import csv\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV files for analysis\n",
    "Note: Running this cell block will append lines to existing CSV files. Delete existing CSV files before each new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"analysis\"):\n",
    "    os.mkdir(\"analysis\")\n",
    "\n",
    "\n",
    "def detect_tracking(blocklist, url_list):\n",
    "    \"\"\"\n",
    "    Check if any URLs from a list appear in a blocklist of known tracking cookies.\n",
    "\n",
    "    Args:\n",
    "        blocklist: Set of blocked domains.\n",
    "        url_list: List of URLs.\n",
    "\n",
    "    Returns:\n",
    "        A list of detected trackers.\n",
    "    \"\"\"\n",
    "\n",
    "    detected_trackers = []\n",
    "    for url in url_list:\n",
    "        if utils.get_full_domain(url) in blocklist:\n",
    "            detected_trackers.append(url)\n",
    "\n",
    "    return detected_trackers\n",
    "\n",
    "\n",
    "def get_urls_from_har(file: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of cookies from an HAR file.\n",
    "    [HAR Specification](http://www.softwareishard.com/blog/har-12-spec/).\n",
    "\n",
    "    Args:\n",
    "        file: Path to the HAR file.\n",
    "    Returns:\n",
    "        A list of cookies.\n",
    "    \"\"\"\n",
    "\n",
    "    all_urls = []\n",
    "    data = json.load(open(file, \"r\")) # parses JSON data into Python dictionary\n",
    "    for entry in data[\"log\"][\"entries\"]: # each entry is an HTTP request/response pair\n",
    "        request = entry[\"request\"] # extract request dictionary\n",
    "\n",
    "        if (url := request.get(\"url\")) and request.get(\"cookies\"): # valid URL exists and request contains cookies\n",
    "            all_urls.append(url)\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "\n",
    "def get_tracking_sites(list_path: str = \"inputs/blocklists/\") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get tracking sites from blocklists.\n",
    "\n",
    "    Args:\n",
    "        list_path: Path to blocklists. Defaults to \"inputs/blocklists/\".\n",
    "\n",
    "    Returns:\n",
    "        A set of tracking sites.\n",
    "    \"\"\"\n",
    "    lists = []\n",
    "    for item in os.listdir(list_path):\n",
    "        path = os.path.join(list_path, item)\n",
    "        lists.append(path)\n",
    "\n",
    "    tracking_sites = set()\n",
    "    for list_path in lists:\n",
    "        with open(list_path) as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                tracking_sites.add(line.rstrip())\n",
    "\n",
    "    # print(\"Tracking sites aggregated from 4 blocklists.\")\n",
    "    return tracking_sites\n",
    "\n",
    "\n",
    "def get_directories(root: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of directories in a given root directory.\n",
    "\n",
    "    Args:\n",
    "        root: Path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of directories.\n",
    "    \"\"\"\n",
    "    dirs = []\n",
    "    for item in os.listdir(root):\n",
    "        path = os.path.join(root, item)\n",
    "        if os.path.isdir(path):\n",
    "            dirs.append(path)\n",
    "\n",
    "    return dirs\n",
    "\n",
    "\n",
    "# Create set of tracking sites from aggregation of 4 blocklists\n",
    "trackings_sites = get_tracking_sites()\n",
    "\n",
    "\n",
    "def analyze_har(har_path: str):\n",
    "    \"\"\"\n",
    "    Return a list of tracking cookies detected in the specified HAR file.\n",
    "\n",
    "    Args:\n",
    "        har_path: Path to the HAR file.\n",
    "\n",
    "    Returns:\n",
    "        A list of detected tracking cookies.\n",
    "    \"\"\"\n",
    "    urls = get_urls_from_har(har_path) # get list of URLs\n",
    "    detected_list = detect_tracking(trackings_sites, urls)\n",
    "    return detected_list\n",
    "\n",
    "\n",
    "success_file_path = \"inputs/sites/success.txt\"\n",
    "with open(success_file_path, \"r\") as success_file:\n",
    "    success_lines = success_file.readlines()\n",
    "\n",
    "domain_paths = get_directories(\"crawls/depth1_noquery\")\n",
    "incomplete_runs = 0\n",
    "total_inner_pages = 0\n",
    "for site in domain_paths:\n",
    "    # Skip if site is not in success.txt\n",
    "    # if not any(site in line for line in success_lines):\n",
    "    #     continue\n",
    "\n",
    "    inner_site_paths = get_directories(site)\n",
    "    total_inner_pages += len(inner_site_paths)\n",
    "\n",
    "    for inner_site_path in inner_site_paths:\n",
    "        normal_har_path = f\"{inner_site_path}/normal.json\"\n",
    "        reject_har_path = f\"{inner_site_path}/after_reject.json\"\n",
    "\n",
    "        if not os.path.isfile(normal_har_path) or not os.path.isfile(reject_har_path):\n",
    "            # Requires both normal and intercept HAR files to exist\n",
    "            incomplete_runs += 1\n",
    "            continue\n",
    "\n",
    "        detected_list_normal = analyze_har(normal_har_path)\n",
    "\n",
    "        # Create file if it doesn't exist; if it exists then write a row for each inner site path with a count of the number of trackers.\n",
    "        normal_file = \"analysis/depth1_noquery_trackers_in_normal.csv\"\n",
    "        normal_file_exists = os.path.isfile(normal_file)\n",
    "\n",
    "        if normal_file_exists:\n",
    "            with open(normal_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([inner_site_path, len(detected_list_normal)])\n",
    "                file.flush() # bugfix where rows weren't writing: flush() clears internal buffer\n",
    "\n",
    "        else:\n",
    "            with open(normal_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "                writer.writerow([inner_site_path, len(detected_list_normal)])\n",
    "                file.flush()\n",
    "\n",
    "\n",
    "        # Repeat for files generated after run with intercept.\n",
    "        detected_list_reject = analyze_har(reject_har_path)\n",
    "\n",
    "        reject_file = \"analysis/depth1_noquery_after_reject.csv\"\n",
    "        reject_file_exists = os.path.isfile(reject_file)\n",
    "\n",
    "        if reject_file_exists:\n",
    "            with open(reject_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([inner_site_path, len(detected_list_reject)])\n",
    "                file.flush()\n",
    "        else:\n",
    "            with open(reject_file, mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Inner Site Path\", \"Length of Detected List\"])\n",
    "                writer.writerow([inner_site_path, len(detected_list_reject)])\n",
    "                file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Tracking Cookies Across All Inner Pages (Regardless of Domain)\n",
    "Run this cell to check that number of complete+incomplete pages equals total inner pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete inner pages: 377\n",
      "Total inner pages: 2160\n",
      "Incomplete inner pages: 1783\n",
      "Inner pages that removed all trackers after rejection: 2\n",
      "Inner pages with increased trackers after rejection: 3\n",
      "Inner pages that never contained trackers: 238\n",
      "Inner pages that sent cookies to 3rd party trackers after rejection: 137\n"
     ]
    }
   ],
   "source": [
    "def compare_trackers():\n",
    "    no_trackers_after_reject = []  # List of inner site paths with trackers in normal crawl, but no trackers after rejection\n",
    "    increased_trackers = []  # List of inner site paths with more trackers after rejection than in normal crawl\n",
    "    never_trackers = []  # List of inner site paths with no trackers in either normal or rejection crawl\n",
    "    violating_sites = []  # List of inner site paths with trackers after we click the reject button\n",
    "\n",
    "    with open('analysis/depth1_noquery_after_reject.csv', 'r') as reject_file, open('analysis/depth1_noquery_trackers_in_normal.csv', 'r') as normal_file:\n",
    "        read_reject = csv.reader(reject_file)\n",
    "        read_normal = csv.reader(normal_file)\n",
    "\n",
    "        # Skip header\n",
    "        next(read_reject)\n",
    "        next(read_normal)\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        # Since both csvs are sorted by inner site path, we can just iterate through both at the same time\n",
    "        for normal, after_reject in zip(read_normal, read_reject):\n",
    "            inner_site_path, num_trackers_normal = normal\n",
    "            _, num_trackers_reject = after_reject\n",
    "\n",
    "            if inner_site_path != _:\n",
    "                raise RuntimeError(\"Inner site paths do not match\")\n",
    "\n",
    "            num_trackers_normal = int(num_trackers_normal)\n",
    "            num_trackers_reject = int(num_trackers_reject)\n",
    "\n",
    "            # length_detected_list_reject = get_length_detected_list(read_reject, inner_site_path)\n",
    "\n",
    "            # site_url = inner_site_path.replace('crawls/', '').replace('/0', '')\n",
    "\n",
    "            if num_trackers_normal > 0 and num_trackers_reject == 0:  # if there are trackers in normal crawl, but not after reject\n",
    "                no_trackers_after_reject.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal < num_trackers_reject:  # if there are more trackers after reject than in normal crawl\n",
    "                increased_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_normal == 0 and num_trackers_reject == 0:  # if there are no trackers in either normal or reject\n",
    "                never_trackers.append(inner_site_path)\n",
    "\n",
    "            if num_trackers_reject != 0:  # if there are trackers in reject\n",
    "                violating_sites.append(inner_site_path)\n",
    "\n",
    "            length += 1\n",
    "\n",
    "    # from previous cell\n",
    "    print(\"Total inner pages:\", total_inner_pages)\n",
    "    print(\"Incomplete inner pages:\", incomplete_runs)\n",
    "    \n",
    "    print(\"Complete inner pages:\", length)\n",
    "    print(\"Inner pages that removed all trackers after rejection:\", len(no_trackers_after_reject))\n",
    "    print(\"Inner pages with increased trackers after rejection:\", len(increased_trackers))\n",
    "    print(\"Inner pages that never contained trackers:\", len(never_trackers))\n",
    "    print(\"Inner pages that sent cookies to 3rd party trackers after rejection:\", len(violating_sites))\n",
    "\n",
    "\n",
    "def get_length_detected_list(csv_reader, inner_site_path):\n",
    "    for row in csv_reader:\n",
    "        current_inner_site_path, length_detected_list = row\n",
    "        if current_inner_site_path == inner_site_path:\n",
    "            return length_detected_list\n",
    "\n",
    "    return '0'  # If inner_site_path not found, return '0'\n",
    "\n",
    "\n",
    "# Call the function to compare the trackers\n",
    "compare_trackers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookie-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
