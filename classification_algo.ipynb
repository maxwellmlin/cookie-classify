{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithm Analysis\n",
    "Notebook to analyze data generated by `classification_algo` in `crawler.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib as mpl\n",
    "from filelock import FileLock\n",
    "from crawler import CrawlResults\n",
    "from utils.utils import get_directories, get_domain, split\n",
    "from utils.image_shingle import ImageShingle\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "CRAWL_NAME = 'KJ2GW'\n",
    "NUM_WORKERS = 25\n",
    "\n",
    "DATA_PATH = Path(\"/usr/project/xtmp/mml66/cookie-classify/\") / CRAWL_NAME\n",
    "ANALYSIS_PATH = Path(\"analysis\") / CRAWL_NAME\n",
    "FIGURE_PATH = Path(\"analysis\") / CRAWL_NAME / \"figs\"\n",
    "FIGURE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "(ANALYSIS_PATH / \"slurm/differences\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config\n",
    "with open(DATA_PATH / \"config.yaml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "# Site list\n",
    "site_list = []\n",
    "with open(config[\"SITE_LIST_PATH\"]) as file:\n",
    "    for line in file:\n",
    "        site_list.append(line.strip())\n",
    "\n",
    "# Site queue\n",
    "queue_lock = FileLock(config[\"QUEUE_PATH\"] + '.lock', timeout=10)\n",
    "with queue_lock:\n",
    "    with open(config[\"QUEUE_PATH\"], 'r') as file:\n",
    "        site_queue = json.load(file)\n",
    "\n",
    "# Site results\n",
    "results_lock = FileLock(config[\"RESULTS_PATH\"] + '.lock', timeout=10)\n",
    "with results_lock:\n",
    "    with open(config[\"RESULTS_PATH\"]) as file:\n",
    "        site_results: dict[str, CrawlResults] = json.load(file)\n",
    "\n",
    "\"\"\"\n",
    "Check crawl completion.\n",
    "\"\"\"\n",
    "print(f\"Crawled {len(site_results)}/{len(site_list)} sites.\")\n",
    "\n",
    "\"\"\"\n",
    "Reduce the number of sites to analyze.\n",
    "A successful site must have:\n",
    "1. a successful domain -> url resolution\n",
    "3. was not terminated via SIGKILL\n",
    "2. no unexpected crawl exceptions\n",
    "\"\"\"\n",
    "successful_sites = []\n",
    "unsuccessful_sites = []\n",
    "keys = set()\n",
    "for domain, result in site_results.items():\n",
    "    keys.update(result.keys())\n",
    "    if result.get(\"url\") and not result.get(\"SIGKILL\") and not result.get(\"unexpected_exception\"):\n",
    "        successful_sites.append(domain)\n",
    "    else:\n",
    "        unsuccessful_sites.append(domain)\n",
    "print(f\"{len(successful_sites)} successful sites.\")\n",
    "\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsuccessful sites stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsuccessful_sites_stats = {\n",
    "    \"landing_page_down\": 0,\n",
    "    \"unexpected_exception\": 0,\n",
    "    \"SIGKILL\": 0,\n",
    "    \"SIGTERM\": 0,\n",
    "}\n",
    "for domain in unsuccessful_sites:\n",
    "    result: CrawlResults = site_results[domain]\n",
    "    for key in unsuccessful_sites_stats.keys():\n",
    "        if result.get(key):\n",
    "            unsuccessful_sites_stats[key] += 1\n",
    "print(unsuccessful_sites_stats)\n",
    "print(f\"Total unsuccessful sites: {len(unsuccessful_sites)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successful sites stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_sites_stats = {\n",
    "    \"landing_page_down\": 0,\n",
    "    \"SIGTERM\": 0,\n",
    "}\n",
    "for domain in successful_sites:\n",
    "    result: CrawlResults = site_results[domain]\n",
    "    for key in successful_sites_stats.keys():\n",
    "        if result.get(key):\n",
    "            successful_sites_stats[key] += 1\n",
    "print(successful_sites_stats)\n",
    "print(f\"Total successful sites: {len(successful_sites)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_keys(original_dict):\n",
    "    \"\"\"\n",
    "    Capitalizes all keys of the given dictionary.\n",
    "\n",
    "    Args:\n",
    "        original_dict (dict): The original dictionary with keys to be capitalized.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with all keys capitalized.\n",
    "    \"\"\"\n",
    "    return {key.capitalize(): value for key, value in original_dict.items()}\n",
    "\n",
    "def generate_pie_chart(freq_dict, title=None):\n",
    "    \"\"\"\n",
    "    Generates a pie chart from a frequency dictionary.\n",
    "\n",
    "    Args:\n",
    "        freq_dict (dict): A dictionary where keys are categories and values are frequencies.\n",
    "        title (str): The title of the pie chart. If None, no title is displayed.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the pie chart.\n",
    "    \"\"\"\n",
    "    freq_dict = capitalize_keys(freq_dict)\n",
    "    \n",
    "    # Sort the keys alphabetically\n",
    "    sorted_keys = sorted(freq_dict.keys())\n",
    "\n",
    "    # Generate a list of distinct colors\n",
    "    # Note: Increase the number of colors if you have more categories\n",
    "    colors = ['gold', 'lightcoral', 'lightskyblue', 'lightgreen', 'lavender', 'orange', 'cyan', 'pink', 'yellowgreen', 'plum']\n",
    "\n",
    "    # Create a color map by assigning each key a color\n",
    "    color_map = {key: colors[i % len(colors)] for i, key in enumerate(sorted_keys)}\n",
    "\n",
    "    # Extract and sort the labels (keys) and sizes (values) from the frequency dictionary based on sorted labels\n",
    "    labels = sorted_keys\n",
    "    sizes = [freq_dict[key] for key in sorted_keys]\n",
    "    \n",
    "    # Get the list of colors based on the order of sorted labels\n",
    "    pie_colors = [color_map[label] for label in labels]\n",
    "\n",
    "    # Generate the pie chart\n",
    "    plt.figure(figsize=(4, 4))  # Set the figure size\n",
    "    plt.pie(sizes, labels=labels, colors=pie_colors, autopct='%1.1f%%', startangle=40)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cdf_df(values: list):\n",
    "    \"\"\"\n",
    "    Returns a dataframe which can be plotted as a CDF graph (see plot_cdf).\n",
    "\n",
    "    args:\n",
    "        values: list of values to plot\n",
    "    \"\"\"\n",
    "    s = pd.Series(values, name = 'value')\n",
    "    df = pd.DataFrame(s)\n",
    "\n",
    "    # Frequency\n",
    "    stats_df = df \\\n",
    "    .groupby('value') \\\n",
    "    ['value'] \\\n",
    "    .agg('count') \\\n",
    "    .pipe(pd.DataFrame) \\\n",
    "    .rename(columns = {'value': 'frequency'})\n",
    "\n",
    "    # PDF\n",
    "    stats_df['pdf'] = stats_df['frequency'] / sum(stats_df['frequency'])\n",
    "\n",
    "    # CDF\n",
    "    stats_df['cdf'] = stats_df['pdf'].cumsum()\n",
    "    stats_df = stats_df.reset_index()\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def plot_cdf(data: dict[str, pd.DataFrame], axis: list):\n",
    "    \"\"\"\n",
    "    Plots multiple CDF graphs on the same axis.\n",
    "\n",
    "    Args:\n",
    "        values_list: list of lists, each containing values to plot\n",
    "        labels: list of labels for the graphs\n",
    "        axis: axis limits\n",
    "    \"\"\"    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for label, values in data.items():\n",
    "        df = get_cdf_df(values)\n",
    "        plt.plot(df['value'], df['cdf'], label=label, alpha=0.8)\n",
    "\n",
    "    plt.axis(axis)\n",
    "\n",
    "    plt.ylabel('CDF')\n",
    "    \n",
    "    plt.grid(visible=True)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_pdf(values: list, label: str, axis: list):\n",
    "    \"\"\"\n",
    "    Plots a PDF graph.\n",
    "\n",
    "    Args:\n",
    "        values: list of values to plot\n",
    "        label: label for the graph\n",
    "        axis: axis limits\n",
    "    \"\"\"    \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "    df = get_cdf_df(values)\n",
    "\n",
    "    plt.plot(df['value'], df['pdf'], label=label)\n",
    "\n",
    "    plt.axis(axis)\n",
    "\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('PDF')\n",
    "\n",
    "    plt.grid(visible = True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for site in site_list:\n",
    "    times.append(site_results[site][\"total_time\"])\n",
    "print(f\"Median time to crawl a site: {statistics.median(times) / 60} minutes.\")\n",
    "print(f\"Total time to crawl all sites: {sum(times) / (60 * 60 * 24)} days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = {}\n",
    "for domain, result in site_results.items():\n",
    "    task_id = result[\"SLURM_ARRAY_TASK_ID\"]\n",
    "    total_time[task_id] = total_time.get(task_id, 0) + (result[\"total_time\"] / (60 * 60))  # convert to hours\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame(list(total_time.items()), columns=['SLURM_ARRAY_TASK_ID', 'Total_Crawl_Time_Hours'])\n",
    "\n",
    "# Sort the DataFrame by 'Total_Crawl_Time' in ascending order\n",
    "df_sorted = df.sort_values(by='Total_Crawl_Time_Hours', ascending=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(df_sorted.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clickstream length\n",
    "Statisistics for both clickstream generation and traversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements clicked by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "click_types = {}\n",
    "for site in successful_sites:\n",
    "    for _, click_type in list(chain.from_iterable(site_results[site]['clickstream'])):\n",
    "        click_types[click_type] = click_types.get(click_type, 0) + 1\n",
    "\n",
    "click_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed elements clicked by type.\n",
    "This means we were able to generate the clickstream, but unable to traverse it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_failures = {}\n",
    "\n",
    "for site in successful_sites:\n",
    "    for type, failures in site_results[site]['traversal_failures'].items():\n",
    "        click_failures[type] = click_failures.get(type, 0) + failures\n",
    "\n",
    "click_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pie_chart(click_types, title=\"All Clickable Elements\")\n",
    "plt.savefig(FIGURE_PATH / f'all_clickable_elements.png', dpi=600, facecolor='white')\n",
    "generate_pie_chart(click_failures, title=\"Failed Clickable Elements\")\n",
    "plt.savefig(FIGURE_PATH / f'failed_clickable_elements.png', dpi=600, facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average clickstream length for each group.\n",
    "\n",
    "- For baseline, this is the average length of the generated clickstream.\n",
    "- For control and experimental, this is the average length of the successful traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_actions(path: Path, name: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of actions in a clickstream by counting the number of images in the directory.\n",
    "    \"\"\"    \n",
    "    count = 0\n",
    "    while (path / f\"{name}-{count+1}.png\").is_file():\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to rebuild\n",
    "# def get_average_clickstream_length_per_site() -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Returns a dataframe with the average number of actions for each site.\n",
    "    \n",
    "#     Should not be used to compute the average clickstream length for the entire crawl\n",
    "#     since we cannot take the average of averages.\n",
    "#     \"\"\"    \n",
    "#     rows_list = []\n",
    "\n",
    "#     for i, domain in enumerate(successful_sites):\n",
    "#         print(f\"Processing site {i+1}/{len(successful_sites)}: {domain}\")\n",
    "#         clickstreams = get_directories(site_results[domain][\"data_path\"])\n",
    "\n",
    "#         baseline_actions = []\n",
    "#         control_actions = []\n",
    "#         experimental_actions = []\n",
    "#         for clickstream in clickstreams:\n",
    "#             if (clickstream / f\"baseline-0.png\").is_file() and \\\n",
    "#                 (clickstream / f\"control-0.png\").is_file() and \\\n",
    "#                     (clickstream / f\"experimental-0.png\").is_file():\n",
    "#                 baseline_actions.append(count_actions(clickstream, \"baseline\"))\n",
    "#                 control_actions.append(count_actions(clickstream, \"control\"))\n",
    "#                 experimental_actions.append(count_actions(clickstream, \"experimental\"))\n",
    "        \n",
    "#         if len(baseline_actions) != 0 and len(control_actions) != 0 and len(experimental_actions) != 0:\n",
    "#             rows_list.append({\n",
    "#                 \"domain\": domain,\n",
    "#                 \"baseline\": statistics.mean(baseline_actions),\n",
    "#                 \"control\": statistics.mean(control_actions),\n",
    "#                 \"experimental\": statistics.mean(experimental_actions)\n",
    "#             })\n",
    "\n",
    "#     return pd.DataFrame(rows_list)\n",
    "\n",
    "# clickstream_lengths = get_average_clickstream_length_per_site()\n",
    "# clickstream_lengths['baseline/control'] = clickstream_lengths['baseline'] / clickstream_lengths['control']\n",
    "\n",
    "# # Sort by baseline/control ratio and remove 0/0\n",
    "# clickstream_length_by_domain = clickstream_lengths.sort_values(by='baseline/control', ascending=False)\n",
    "# clickstream_length_by_domain.dropna(inplace=True)\n",
    "# clickstream_length_by_domain.to_csv(ANALYSIS_PATH / \"clickstream_length_by_domain.csv\", index=False)\n",
    "# clickstream_length_by_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_length_by_domain = pd.read_csv(ANALYSIS_PATH / 'clickstream_length_by_domain.csv')\n",
    "clickstream_length_by_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to rebuild\n",
    "\n",
    "# def get_all_clickstream_lengths() -> tuple[list, list, list]:\n",
    "#     \"\"\"\n",
    "#     Returns a tuple of lists containing the lengths of all clickstreams for each type.\n",
    "\n",
    "#     Returns: baseline_actions, controls_actions, experimental_actions\n",
    "#     \"\"\"\n",
    "#     baseline_actions = []\n",
    "#     control_actions = []\n",
    "#     experimental_actions = []\n",
    "#     for i, domain in enumerate(successful_sites):\n",
    "#         print(f\"Processing site {i+1}/{len(successful_sites)}: {domain}\")\n",
    "        \n",
    "#         clickstreams = get_directories(site_results[domain][\"data_path\"])\n",
    "\n",
    "#         for clickstream in clickstreams:\n",
    "#             # Ensure that we've navigated to the landing page for all three groups\n",
    "#             if (clickstream / f\"baseline-0.png\").is_file() and \\\n",
    "#                     (clickstream / f\"control-0.png\").is_file() and \\\n",
    "#                         (clickstream / f\"experimental-0.png\").is_file():\n",
    "#                 baseline_actions.append(count_actions(clickstream, \"baseline\"))\n",
    "#                 control_actions.append(count_actions(clickstream, \"control\"))\n",
    "#                 experimental_actions.append(count_actions(clickstream, \"experimental\"))\n",
    "    \n",
    "#     return baseline_actions, control_actions, experimental_actions\n",
    "\n",
    "# baseline_actions, control_actions, experimental_actions = get_all_clickstream_lengths()\n",
    "\n",
    "# with open(ANALYSIS_PATH / 'baseline_clickstream_lengths.json', 'w') as f:\n",
    "#     json.dump(baseline_actions, f)\n",
    "# with open(ANALYSIS_PATH / 'control_clickstream_lengths.json', 'w') as f:\n",
    "#     json.dump(control_actions, f)\n",
    "# with open(ANALYSIS_PATH / 'experimental_clickstream_lengths.json', 'w') as f:\n",
    "#     json.dump(experimental_actions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ANALYSIS_PATH / 'baseline_clickstream_lengths.json', 'r') as f:\n",
    "    baseline_actions = json.load(f)\n",
    "with open(ANALYSIS_PATH / 'control_clickstream_lengths.json', 'r') as f:\n",
    "    control_actions = json.load(f)\n",
    "with open(ANALYSIS_PATH / 'experimental_clickstream_lengths.json', 'r') as f:\n",
    "    experimental_actions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average baseline length: {statistics.mean(baseline_actions)}\")\n",
    "print(f\"Average control length: {statistics.mean(control_actions)}\")\n",
    "print(f\"Average experimental length: {statistics.mean(experimental_actions)}\")\n",
    "\n",
    "def plot_normalized_frequencies(lists_of_numbers, labels):\n",
    "    \"\"\"\n",
    "    Plots the normalized frequencies of numbers as percentages from multiple lists with custom labels,\n",
    "    with reduced whitespace between each set of bars for a more compact visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    lists_of_numbers (list of list of int): A list containing multiple lists of numbers.\n",
    "    labels (list of str): A list containing labels for each list of numbers.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store normalized frequencies for each list\n",
    "    all_normalized_frequencies = []\n",
    "\n",
    "    # Function to calculate frequency\n",
    "    def calculate_frequency(numbers):\n",
    "        frequency = {}\n",
    "        for number in numbers:\n",
    "            if number in frequency:\n",
    "                frequency[number] += 1\n",
    "            else:\n",
    "                frequency[number] = 1\n",
    "        return frequency\n",
    "\n",
    "    # Function to normalize frequency\n",
    "    def normalize_frequency(frequency):\n",
    "        total_count = sum(frequency.values())\n",
    "        return {number: (count / total_count) for number, count in frequency.items()}\n",
    "\n",
    "    # Calculating and normalizing frequencies for each list\n",
    "    for numbers_list in lists_of_numbers:\n",
    "        frequency = calculate_frequency(numbers_list)\n",
    "        normalized_frequency = normalize_frequency(frequency)\n",
    "        all_normalized_frequencies.append(normalized_frequency)\n",
    "\n",
    "    # Combining all numbers from all lists and sorting them\n",
    "    all_numbers = sorted(set().union(*[frequency.keys() for frequency in all_normalized_frequencies]))\n",
    "\n",
    "    # Preparing the plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Adjusting the width for reduced whitespace\n",
    "    width = 0.35\n",
    "    indices = [x * (1 + width) for x in range(len(all_numbers))]\n",
    "\n",
    "    for i, normalized_frequency in enumerate(all_normalized_frequencies):\n",
    "        data = [normalized_frequency.get(number, 0) for number in all_numbers]\n",
    "        plt.bar([index + (i * width) - (width * len(lists_of_numbers) / 2) + width / 2 for index in indices], data, width=width, label=labels[i] if i < len(labels) else f'List {i+1}')\n",
    "\n",
    "    plt.xlabel('Clickstream Length')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Clickstream Length across Crawl Groups')\n",
    "    plt.yticks(np.arange(0, 1+0.1, 0.1))\n",
    "    plt.xticks(indices, all_numbers)\n",
    "    plt.legend().set_title(\"Crawl Group\")\n",
    "    plt.grid(axis='y')\n",
    "\n",
    "    # Showing the plot\n",
    "    plt.savefig(FIGURE_PATH / 'clickstream_length.png', bbox_inches='tight', dpi=600, facecolor='white')\n",
    "plot_normalized_frequencies([baseline_actions, control_actions, experimental_actions], [\"Baseline\", \"Control\", \"Experimental\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_differences = {}\n",
    "for i in range(NUM_WORKERS):\n",
    "    with open(ANALYSIS_PATH / f'slurm/differences/{i}.json', 'r') as file:\n",
    "        to_merge = json.load(file)\n",
    "    raw_differences.update(to_merge)\n",
    "print(f\"Successful domains extracted: {len(raw_differences)}/{len(successful_sites)}\")\n",
    "\n",
    "def unravel(data: dict, diff_type: str, add_labels: bool = False) -> tuple[list, dict]:\n",
    "    \"\"\"\n",
    "    Unravel the screenshot differences, filtered by clicksteam depth.\n",
    "\n",
    "    Args:\n",
    "        screenshots: The screenshots dictionary. See screenshot_comparison.py for the schema.\n",
    "        difference_type: The type of difference to unravel.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[list, dict]: A tuple containing a list of all the differences and a dictionary containing the differences by clickstream depth.\n",
    "    \"\"\"\n",
    "    # Init\n",
    "    # Schema: depth -> [domain, difference] | [difference]\n",
    "    unraveled_raw: dict[int, pd.DataFrame | list] = {}  # Unravel each difference by depth\n",
    "    unraveled_domains: dict[int, pd.DataFrame | list] = {}  #  # Average differences by domain before unraveling\n",
    "    for i in list(range(config[\"CLICKSTREAM_LENGTH\"]+1)) + [\"ALL\"]:\n",
    "        unraveled_raw[i] = []\n",
    "        unraveled_domains[i] = []\n",
    "\n",
    "    # Unravel\n",
    "    for domain, clickstreams in data.items():\n",
    "        domain_diffs = {}  # collect all diffs for a given domain by depth\n",
    "        for clickstream, actions in clickstreams.items():\n",
    "            for action, diff_dict in actions.items():\n",
    "                action = int(action)\n",
    "                \n",
    "                if diff_type in diff_dict:\n",
    "                    # Append to domain_diffs\n",
    "                    domain_diffs[action] = domain_diffs.get(action, []) + [diff_dict[diff_type]]\n",
    "                    domain_diffs[\"ALL\"] = domain_diffs.get(\"ALL\", []) + [diff_dict[diff_type]]\n",
    "\n",
    "        # Unravel all diffs into results\n",
    "        for depth, diffs in domain_diffs.items():\n",
    "            if add_labels:\n",
    "                unraveled_raw[depth].extend([{\"domain\": domain, diff_type: diff} for diff in diffs])\n",
    "                unraveled_domains[depth].append({\"domain\": domain, diff_type: statistics.mean(diffs)})\n",
    "            else:\n",
    "                unraveled_raw[depth].extend(diffs)\n",
    "                unraveled_domains[depth].append(statistics.mean(diffs))\n",
    "                \n",
    "    # Convert dict to dataframe\n",
    "    if add_labels:\n",
    "        for depth, diffs in unraveled_domains.items():\n",
    "            unraveled_domains[depth] = pd.DataFrame(diffs)\n",
    "        for depth, diffs in unraveled_raw.items():\n",
    "            unraveled_raw[depth] = pd.DataFrame(diffs)\n",
    "        \n",
    "    return {\"unraveled_raw\": unraveled_raw, \"unraveled_domains\": unraveled_domains}\n",
    "\n",
    "diff_types = [\"bce_diff\", \"shingle_did\", \"img_did\", \"innerText_did\", \"links_did\"]\n",
    "\n",
    "unraveled_raw = {}\n",
    "unraveled_domains = {}\n",
    "for diff_type in diff_types:\n",
    "    unraveled_raw[diff_type] = unravel(raw_differences, diff_type)[\"unraveled_raw\"]\n",
    "    unraveled_domains[diff_type] = unravel(raw_differences, diff_type)[\"unraveled_domains\"]\n",
    "    \n",
    "diffs_df = pd.DataFrame(list(successful_sites), columns=['domain'])\n",
    "for diff_type in diff_types:\n",
    "    to_merge = unravel(raw_differences, diff_type, add_labels=True)[\"unraveled_domains\"][\"ALL\"]\n",
    "    diffs_df = diffs_df.merge(to_merge, on='domain', how='left')\n",
    "\n",
    "diffs_df.to_csv(ANALYSIS_PATH / \"differences.csv\", index=False)\n",
    "diffs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshot Difference CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    \"bce_diff\": \"Screenshot\",\n",
    "}\n",
    "\n",
    "for diff_type in names:\n",
    "    axis = [0, 0.5, 0, 1]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for label, values in unraveled_domains[diff_type].items():\n",
    "        df = get_cdf_df(values)\n",
    "        plt.plot(df['value'], df['cdf'], label=label, alpha=0.8)\n",
    "    plt.axis(axis)\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.yticks(np.arange(0, 1+0.1, 0.1))\n",
    "    plt.grid(visible=True)\n",
    "    plt.legend()\n",
    "    plt.title(names[diff_type])\n",
    "    plt.xlabel('BCE Screenshot Difference')\n",
    "    plt.legend().set_title(\"Depth\")\n",
    "    plt.savefig(FIGURE_PATH / f'{diff_type}.png', bbox_inches='tight', dpi=600, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    \"shingle_did\": \"Image Shingles\",\n",
    "    \"innerText_did\": \"Inner Text\",\n",
    "    \"img_did\": \"Images\",\n",
    "    \"links_did\": \"Links\",\n",
    "}\n",
    "\n",
    "# Setup the figure and axes for a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # Adjust the size as needed\n",
    "axs = axs.flatten()  # Flatten to ease the iteration\n",
    "\n",
    "for i, (diff_type, name) in enumerate(names.items()):\n",
    "    axis = [-0.2, 0.2, 0, 1]\n",
    "    axs[i].set_title(name)\n",
    "    axs[i].set_xlabel('Difference in Difference')\n",
    "    axs[i].set_ylabel('Cumulative Probability')\n",
    "    axs[i].set_yticks(np.arange(0, 1+0.1, 0.1))\n",
    "    axs[i].grid(visible=True)\n",
    "    axs[i].axis(axis)\n",
    "    \n",
    "    for label, values in unraveled_domains[diff_type].items():\n",
    "        df = get_cdf_df(values)\n",
    "        axs[i].plot(df['value'], df['cdf'], label=label, alpha=0.8)\n",
    "        \n",
    "    axs[i].legend(loc='upper left').set_title(\"Depth\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the combined figure\n",
    "plt.savefig(FIGURE_PATH / 'combined_jaccard_did.png', bbox_inches='tight', dpi=600, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "names = {\n",
    "    \"shingle_did\": \"Image Shingles\",\n",
    "    \"innerText_did\": \"Inner Text\",\n",
    "    \"img_did\": \"Images\",\n",
    "    \"links_did\": \"Links\",\n",
    "}\n",
    "\n",
    "# Setup the figure and axes for a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))  # Adjust the size as needed\n",
    "axs = axs.flatten()  # Flatten to ease the iteration\n",
    "\n",
    "for i, (diff_type, label) in enumerate(names.items()):\n",
    "    data = unraveled_domains[diff_type][\"ALL\"]\n",
    "\n",
    "    # Fit a normal distribution to the data:\n",
    "    mu, std = norm.fit(data)\n",
    "\n",
    "    # Plot the histogram on subplot\n",
    "    axs[i].hist(data, bins=50, density=True, alpha=0.6, label='Histogram')\n",
    "\n",
    "    # Plot the PDF on subplot\n",
    "    xmin, xmax = axs[i].get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    axs[i].plot(x, p, 'k', linewidth=2, label='Gaussian Fit: μ=%.2f, σ=%.2f' % (mu, std))\n",
    "    \n",
    "    # Setting title, labels, and legend\n",
    "    axs[i].set_title(label)\n",
    "    axs[i].set_xlabel('Difference in Difference')\n",
    "    axs[i].set_ylabel('Probability Density')\n",
    "    axs[i].legend(loc='upper left')\n",
    "    axs[i].axis([-1, 1, 0, 35])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(FIGURE_PATH / 'combined_fit_gaussian.png', bbox_inches='tight', dpi=600, facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cookie-classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
